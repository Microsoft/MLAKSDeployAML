{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Locally\n",
    "In this notebook, you will perform the following using Azure Machine Learning.\n",
    "* Load workspace.\n",
    "* Configure & execute a local run in a user-managed Python environment.\n",
    "* Configure & execute a local run in a system-managed Python environment.\n",
    "* Configure & execute a local run in a Docker environment.\n",
    "* Register model for operationalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core import Experiment\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from dotenv import set_key, get_key, find_dotenv\n",
    "from utilities import get_auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = find_dotenv(raise_error_if_not_found=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses a training script that uses [lightgbm](https://lightgbm.readthedocs.io/en/latest/Python-API.html#scikit-learn-api) . Here we set the number of estimators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "num_estimators = \"10\"\n",
    "set_key(env_path, \"num_estimators\", num_estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "\n",
    "Initialize a workspace object from persisted configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config(auth=get_auth(env_path))\n",
    "print(ws.name, ws.resource_group, ws.location, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create An Experiment\n",
    "**Experiment** is a logical container in an Azure ML Workspace. It hosts run records which can include run metrics and output artifacts from your experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"mlaks-train-on-local\"\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View `create_model.py`\n",
    "\n",
    "The script that trains the model `create_model.py` is already created for you. Let's check its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./scripts/create_model.py\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note `create_model.py` also references a `item_selector.py` and `label_rank.py` file. Let's check those scripts as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./scripts/item_selector.py', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./scripts/label_rank.py', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure & Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we show three different ways of locally training your model through Azure ML SDK for demonstration purposes. Only one of these runs is sufficient to register the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### User-managed environment\n",
    "Below, we use a user-managed run, which means you are responsible to ensure all the necessary packages that are available in the Python environment you choose to run the script. We will use the environment created for this tutorial which has Azure ML SDK and other dependencies installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Editing a run configuration property on-fly.\n",
    "run_config_user_managed = RunConfiguration()\n",
    "\n",
    "run_config_user_managed.environment.python.user_managed_dependencies = True\n",
    "\n",
    "# Choose the specific Python environment of this tutorial by pointing to the Python path\n",
    "run_config_user_managed.environment.python.interpreter_path = (\n",
    "    \"/anaconda/envs/MLAKSDeployAML/bin/python\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit script to run in the user-managed environment\n",
    "Note that the whole `scripts` folder is submitted for execution, including the `item_selector.py` and `label_rank.py` files. The model will be written to `outputs` directory which is a special directory such that all content in this directory is automatically uploaded to your workspace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrpt = \"create_model.py\"\n",
    "args = [\n",
    "    \"--inputs\",\n",
    "    os.path.abspath(\"./data_folder\"),\n",
    "    \"--outputs\",\n",
    "    \"outputs\",\n",
    "    \"--estimators\",\n",
    "    get_key(env_path, 'num_estimators'),\n",
    "    \"--match\",\n",
    "    \"5\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = ScriptRunConfig(\n",
    "    source_directory=\"./scripts\",\n",
    "    script=scrpt,\n",
    "    arguments=args,\n",
    "    run_config=run_config_user_managed,\n",
    ")\n",
    "#run = exp.submit(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get run history details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Block to wait till run finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the model is now available in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run.get_file_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrive the accuarcy of the model from run logs by querying the run metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run.get_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System-managed environment\n",
    "You can also ask the system to build a new conda environment and execute your scripts in it. The environment is built once and will be reused in subsequent executions as long as the conda dependencies remain unchanged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config_system_managed = RunConfiguration()\n",
    "run_config_system_managed.environment.python.user_managed_dependencies = False\n",
    "run_config_system_managed.auto_prepare_environment = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's specifiy the conda and pip dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify conda dependencies with scikit-learn and pandas\n",
    "conda_pack = [\"scikit-learn==0.19.1\", \"pandas==0.23.3\"]\n",
    "requirements = [\"lightgbm==2.1.2\", \"azureml-defaults==1.0.57\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = CondaDependencies.create(conda_packages=conda_pack, pip_packages=requirements)\n",
    "run_config_system_managed.environment.python.conda_dependencies = cd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit script to run in the system-managed environment\n",
    "A new conda environment is built based on the conda dependencies object. If you are running this for the first time,  this might take up to 5 minutes. But this conda environment is reused so long as you don't change the conda dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = ScriptRunConfig(\n",
    "    source_directory=\"./scripts\",\n",
    "    script=scrpt,\n",
    "    arguments=args,\n",
    "    run_config=run_config_system_managed,\n",
    ")\n",
    "run = exp.submit(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get run history details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Block and wait till run finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "run.wait_for_completion(show_output = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_file_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docker-based execution\n",
    "**IMPORTANT**: You must have Docker engine installed locally in order to use this execution mode. If your kernel is already running in a Docker container, such as **Azure Notebooks**, this mode will **NOT** work.\n",
    "\n",
    "You can also ask the system to pull down a Docker image and execute your scripts in it. We will use the `continuumio/miniconda3` image for that purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config_docker = RunConfiguration()\n",
    "run_config_docker.environment.python.user_managed_dependencies = False\n",
    "run_config_docker.auto_prepare_environment = True\n",
    "run_config_docker.environment.docker.enabled = True\n",
    "run_config_docker.environment.docker.base_image = \"continuumio/miniconda3\"\n",
    "\n",
    "# Specify conda and pip dependencies\n",
    "cd = CondaDependencies.create(conda_packages=conda_pack, pip_packages=requirements)\n",
    "run_config_docker.environment.python.conda_dependencies = cd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we map the local `data_folder` that includes the training and testing data to the docker container using `-v` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_dir = os.path.abspath(\"./data_folder\")\n",
    "container_dir = \"/data_folder\"\n",
    "docker_arg = \"{}:{}\".format(host_dir, container_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time the run will use the mapped `data_folder` inside the docker container to find the data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = [\n",
    "    \"--inputs\",\n",
    "    \"/data_folder\",\n",
    "    \"--outputs\",\n",
    "    \"outputs\",\n",
    "    \"--estimators\",\n",
    "    get_key(env_path, 'num_estimators'),\n",
    "    \"--match\",\n",
    "    \"5\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config_docker.environment.docker.arguments.append(\"-v\")\n",
    "run_config_docker.environment.docker.arguments.append(docker_arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = ScriptRunConfig(\n",
    "    source_directory=\"./scripts\",\n",
    "    script=scrpt,\n",
    "    arguments=args,\n",
    "    run_config=run_config_docker,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " run = exp.submit(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get run history details\n",
    "run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_file_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.get_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now register the model with the workspace so that we can later deploy the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supply a model name, and the full path to the serialized model file.\n",
    "model = run.register_model(\n",
    "    model_name=\"question_match_model\", model_path=\"./outputs/model.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.name, model.version, model.url, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version = str(model.version)\n",
    "set_key(env_path, \"model_version\", model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we [develop the scoring script](03_DevelopScoringScript.ipynb) for this model."
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "roastala"
   }
  ],
  "kernelspec": {
   "display_name": "Python [conda env:MLAKSDeployAML]",
   "language": "python",
   "name": "conda-env-MLAKSDeployAML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
